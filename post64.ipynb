{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Wy_-rsXM8mFm6kOroifcncgd2LiXbuuY","timestamp":1671208761950}],"authorship_tag":"ABX9TyOEFvFrdjYRJVfI6aUlgn8N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Source: https://github.com/Ahmednull/L2CS-Net"],"metadata":{"id":"jkThpPm8nAkn"}},{"cell_type":"code","source":["if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount('/content/drive/')\n","\n","    import os\n","    os.chdir('/content/drive/MyDrive/Colab Notebooks/L2CS')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cf50bc_0CHwW","executionInfo":{"status":"ok","timestamp":1671206580034,"user_tz":-60,"elapsed":5250,"user":{"displayName":"Bertrand Schneider","userId":"01755831888995039676"}},"outputId":"58f05690-1a1d-471b-d227-00cd5b0c5c60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ReI8YztOCBF5","executionInfo":{"status":"ok","timestamp":1671206582810,"user_tz":-60,"elapsed":2783,"user":{"displayName":"Bertrand Schneider","userId":"01755831888995039676"}},"outputId":"4409a104-ae1d-48ee-be31-03dd57949a03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (3.6.2)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.21.6)\n","Requirement already satisfied: opencv-python>=4.5.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (4.6.0.66)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.3.5)\n","Requirement already satisfied: Pillow>=8.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (9.3.0)\n","Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.7.3)\n","Requirement already satisfied: torch>=1.10.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.13.0+cu116)\n","Requirement already satisfied: torchvision>=0.11.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (0.14.0+cu116)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (4.38.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (1.4.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (1.0.6)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->-r requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->-r requirements.txt (line 4)) (2022.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.1->-r requirements.txt (line 7)) (4.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.11.2->-r requirements.txt (line 8)) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->-r requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.11.2->-r requirements.txt (line 8)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.11.2->-r requirements.txt (line 8)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.11.2->-r requirements.txt (line 8)) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.11.2->-r requirements.txt (line 8)) (2.10)\n"]}],"source":["! pip install -r requirements.txt  "]},{"cell_type":"code","source":["! pip install git+https://github.com/elliottzheng/face-detection.git@master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFDIdrelCCV8","executionInfo":{"status":"ok","timestamp":1671206587512,"user_tz":-60,"elapsed":4714,"user":{"displayName":"Bertrand Schneider","userId":"01755831888995039676"}},"outputId":"9ec929eb-4086-441e-d91f-779eaa1cc342"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/elliottzheng/face-detection.git@master\n","  Cloning https://github.com/elliottzheng/face-detection.git (to revision master) to /tmp/pip-req-build-8st7hn6_\n","  Running command git clone -q https://github.com/elliottzheng/face-detection.git /tmp/pip-req-build-8st7hn6_\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from face-detection==1.0.5) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from face-detection==1.0.5) (1.13.0+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from face-detection==1.0.5) (0.14.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->face-detection==1.0.5) (4.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->face-detection==1.0.5) (2.23.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->face-detection==1.0.5) (9.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->face-detection==1.0.5) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->face-detection==1.0.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->face-detection==1.0.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->face-detection==1.0.5) (3.0.4)\n"]}]},{"cell_type":"code","source":["import argparse\n","import numpy as np\n","import cv2\n","import time\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torchvision import transforms\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","\n","from PIL import Image\n","from utils import select_device, draw_gaze\n","from PIL import Image, ImageOps\n","\n","from face_detection import RetinaFace\n","from model import L2CS\n","\n","from google.colab.patches import cv2_imshow"],"metadata":{"id":"NZZdtHI2zlGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def draw_gaze_on_frame(frame):\n","  faces = detector(frame)\n","  if faces is not None: \n","      for box, landmarks, score in faces:\n","          if score < .95:\n","              continue\n","          x_min=int(box[0])\n","          if x_min < 0:\n","              x_min = 0\n","          y_min=int(box[1])\n","          if y_min < 0:\n","              y_min = 0\n","          x_max=int(box[2])\n","          y_max=int(box[3])\n","          bbox_width = x_max - x_min\n","          bbox_height = y_max - y_min\n","\n","          # Crop image\n","          img = frame[y_min:y_max, x_min:x_max]\n","          img = cv2.resize(img, (224, 224))\n","          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","          im_pil = Image.fromarray(img)\n","          img=transformations(im_pil)\n","          img  = Variable(img).cuda(gpu)\n","          img  = img.unsqueeze(0) \n","          \n","          # gaze prediction\n","          gaze_pitch, gaze_yaw = model(img)\n","          \n","          \n","          pitch_predicted = softmax(gaze_pitch)\n","          yaw_predicted = softmax(gaze_yaw)\n","          \n","          # Get continuous predictions in degrees.\n","          pitch_predicted = torch.sum(pitch_predicted.data[0] * idx_tensor) * 4 - 180\n","          yaw_predicted = torch.sum(yaw_predicted.data[0] * idx_tensor) * 4 - 180\n","          \n","          pitch_predicted= pitch_predicted.cpu().detach().numpy()* np.pi/180.0\n","          yaw_predicted= yaw_predicted.cpu().detach().numpy()* np.pi/180.0\n","\n","          draw_gaze(x_min,y_min,bbox_width, bbox_height,frame,(pitch_predicted,yaw_predicted),color=(0,0,255))\n","          cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 1)\n","  return frame"],"metadata":{"id":"zH3p3YP_OX1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cudnn.enabled = True\n","arch= 'ResNet50'\n","batch_size = 1\n","cam = 0\n","gpu = select_device(0, batch_size=batch_size)\n","snapshot_path = 'models/L2CSNet_gaze360.pkl'\n","\n","transformations = transforms.Compose([\n","    transforms.Resize(448),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","model=getArch(arch, 90)\n","print('Loading snapshot.')\n","saved_state_dict = torch.load(snapshot_path)\n","model.load_state_dict(saved_state_dict)\n","model.cuda(gpu)\n","model.eval()\n","\n","\n","softmax = nn.Softmax(dim=1)\n","detector = RetinaFace(gpu_id=0)\n","idx_tensor = [idx for idx in range(90)]\n","idx_tensor = torch.FloatTensor(idx_tensor).cuda(gpu)\n","x=0\n","\n","print('opening video')\n","cap = cv2.VideoCapture(\"bertrand.mp4\")\n","\n","w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","video_path = \"bertrand-gaze.avi\"\n","fourcc = cv2.VideoWriter_fourcc('H','2','6','4')\n","fourcc = cv2.VideoWriter_fourcc('F','M','P','4')\n","fps = float(cap.get(cv2.CAP_PROP_FPS))\n","video = cv2.VideoWriter(video_path, fourcc, fps, (w,h)) \n","video = cv2.VideoWriter(video_path, fourcc, 15.0, (1080,720))\n","\n","# Check if the webcam is opened correctly\n","if not cap.isOpened():\n","    raise IOError(\"Cannot open webcam\")\n","\n","print('process each video frame')\n","with torch.no_grad():\n","    i = 0\n","    while True:\n","        success, frame = cap.read()\n","        i += 1\n","        if not success: continue; print('failed')\n","        start_fps = time.time()\n","        frame = draw_gaze_on_frame(frame)\n","\n","        myFPS = 1.0 / (time.time() - start_fps)\n","        cv2.putText(frame, 'FPS: {:.1f}'.format(myFPS), (10, 20),cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 255, 0), 1, cv2.LINE_AA)\n","\n","        print(str(i), end=',')\n","        video.write(frame)\n","\n","    video.release()"],"metadata":{"id":"wHHTgnDtGPNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! ffmpeg -i bertrand-gaze.avi -vcodec h264 -acodec mp2 bertrand-gaze.mp4"],"metadata":{"id":"0vIcA98jGQmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import HTML\n","from base64 import b64encode\n","mp4 = open('bertrand-gaze.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"metadata":{"id":"SbiT4CDYGY7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Real time webcam feed"],"metadata":{"id":"sPEVai6RGTUC"}},{"cell_type":"code","source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"-kvzV8bXF7if"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"ycsXtxQDFuo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"metadata":{"id":"jc1fWhuBFxpT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # get face region coordinates\n","    faces = face_cascade.detectMultiScale(gray)\n","    # get face bounding box for overlay\n","    for (x,y,w,h) in faces:\n","      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes"],"metadata":{"id":"5lhwovteF2oP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cudnn.enabled = True\n","arch= 'ResNet50'\n","batch_size = 1\n","cam = 0\n","gpu = select_device(0, batch_size=batch_size)\n","snapshot_path = 'models/L2CSNet_gaze360.pkl'\n","\n","transformations = transforms.Compose([\n","    transforms.Resize(448),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n","model=getArch(arch, 90)\n","print('Loading snapshot.')\n","saved_state_dict = torch.load(snapshot_path)\n","model.load_state_dict(saved_state_dict)\n","model.cuda(gpu)\n","model.eval()\n","\n","\n","softmax = nn.Softmax(dim=1)\n","detector = RetinaFace(gpu_id=0)\n","idx_tensor = [idx for idx in range(90)]\n","idx_tensor = torch.FloatTensor(idx_tensor).cuda(gpu)\n","x=0\n","\n","# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","\n","\n","with torch.no_grad():\n","    i = 0\n","    while True:\n","\n","        # get the video frame\n","        js_reply = video_frame(label_html, bbox)\n","        if not js_reply:\n","            break\n","            \n","        # convert JS response to OpenCV Image\n","        frame = js_to_image(js_reply[\"img\"])\n","\n","        # create transparent overlay for bounding box\n","        bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","        # draw the face and gaze\n","        start_fps = time.time()\n","        frame = draw_gaze_on_frame(frame)\n","\n","        myFPS = 1.0 / (time.time() - start_fps)\n","        cv2.putText(frame, 'FPS: {:.1f}'.format(myFPS), (10, 20),cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 255, 0), 1, cv2.LINE_AA)\n","\n","        bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","        # convert overlay of bbox into bytes\n","        bbox_bytes = bbox_to_bytes(bbox_array)\n","        # update bbox so next frame gets new overlay\n","        bbox = bbox_bytes   \n","\n","\n","\n","\n","while True:\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # get face region coordinates\n","    faces = face_cascade.detectMultiScale(gray)\n","    \n","    # get face bounding box for overlay\n","    for (x,y,w,h) in faces:\n","      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes"],"metadata":{"id":"ewp_8-WFzwf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PsrRW5GZxHqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8_JhLprpuJ7P"},"execution_count":null,"outputs":[]}]}